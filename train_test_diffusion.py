#
# Script for training a conditional diffusion decoder (UNet)
# to reconstruct images. The conditioning input is derived from the
# ViT latent space based on I-JEPA's multi-block context masks,
# and the loss is applied only to the target (masked) pixel regions.
#
# Usage:
# python ijepa_diffusion_train.py --input_dir /path/to/your/imagenet/data --mode train
# python ijepa_diffusion_train.py --input_dir /path/to/data --mode inference --checkpoint ./diffusion_checkpoints_ijepa/checkpoint_epoch_100.pth
#
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
import timm
from diffusers import DDPMScheduler
from tqdm import tqdm
import argparse
import os
import numpy as np
import sys
from PIL import Image
from torch.utils.data import Dataset, DataLoader # Needed for the fixed data loader

# --- I-JEPA Dependencies (Simplified Stubs for required components) ---
# NOTE: These stubs simulate the data and mask structure generated by the
#       official I-JEPA loader, assuming a single process (rank 0, world size 1).

class DummyLogger:
    def info(self, msg): print(msg)

class DummySampler:
    def set_epoch(self, e): pass

class MBMaskCollator:
    """Simulated Mask Collator from I-JEPA."""
    def __init__(self, **kwargs): pass
    def __call__(self, batch): return batch
    def step(self): pass

def make_transforms(crop_size, crop_scale, gaussian_blur, horizontal_flip, color_distortion, color_jitter):
    """Generates the standard I-JEPA transform pipeline."""
    transform_list = [
        T.RandomResizedCrop(crop_size, scale=crop_scale),
    ]
    if horizontal_flip:
        transform_list.append(T.RandomHorizontalFlip())
    # Note: Color distortion and Gaussian blur logic would go here
    transform_list.extend([
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return T.Compose(transform_list)

# --- CORRECTED DATASET LOADING CLASS ---
class ImageDataset(Dataset):
    """
    Loads actual images from the directory specified by root_dir. 
    Falls back to mock data if no images are found.
    """
    def __init__(self, root_dir, transform):
        self.transform = transform
        self.root_dir = root_dir
        
        self.image_files = []
        if os.path.isdir(root_dir):
            for entry in os.listdir(root_dir):
                if entry.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):
                    self.image_files.append(os.path.join(root_dir, entry))
        
        self.len = len(self.image_files)
        if self.len == 0:
            print(f"Warning: No images found in '{root_dir}'. Using mock data generation for 16 samples.")
            self.len = 16
            self._mock_mode = True
        else:
            print(f"Found {self.len} images in '{root_dir}'. Using real data.")
            self._mock_mode = False

    def __len__(self): return self.len

    def __getitem__(self, idx): 
        if self._mock_mode:
            # Fallback to Mock data (random image)
            # Create a simple colored square placeholder
            raw_img = Image.new('RGB', (224, 224), color = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)))
        else:
            # Load actual image
            path = self.image_files[idx % len(self.image_files)] 
            try:
                # Open and ensure RGB format
                raw_img = Image.open(path).convert("RGB")
            except Exception as e:
                print(f"Error loading image {path}: {e}. Using fallback mock image.")
                raw_img = Image.new('RGB', (224, 224), color = (255, 0, 0)) # Red placeholder

        # Apply transformations (RandomResizedCrop, ToTensor, Normalize)
        img = self.transform(raw_img)
        
        # Mock mask generation (Normalized coordinates x, y, w, h)
        # Simulates a fixed masking pattern for simplicity in this example
        masks_enc = [
            torch.tensor([[0.0, 0.0, 0.4, 0.4], [0.6, 0.6, 0.4, 0.4]]) # Context
        ]
        masks_pred = [
            torch.tensor([[0.6, 0.0, 0.4, 0.4], [0.0, 0.6, 0.4, 0.4]]) # Target
        ]
        
        # The I-JEPA loader yields (udata, masks_enc, masks_pred)
        return (img,), masks_enc, masks_pred


def make_imagenet1k(transform, batch_size, collator, **kwargs): 
    """Creates the data loader, now using actual images if available."""
    
    dataset = ImageDataset(kwargs['root_path'], transform)
    
    if len(dataset) == 0:
        return None, None, DummySampler()
        
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=kwargs['num_workers'])
    return dataset, loader, DummySampler()
# --- End of CORRECTED DATASET LOADING CLASS ---

# --- Configuration and Arguments ---

parser = argparse.ArgumentParser(description="Train/Inference Mask-Conditioned Diffusion Decoder")
parser.add_argument('--mode', type=str, default='train', choices=['train', 'inference'], help='Operation mode.')
parser.add_argument('--input_dir', type=str, required=True, help='Root directory for the dataset (e.g., ImageNet path).')
parser.add_argument('--save_dir', type=str, default='./diffusion_checkpoints_ijepa', help='Directory to save/load checkpoints and inference results.')
parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')
parser.add_argument('--batch_size', type=int, default=32, help='Training batch size.')
parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate for optimizer.')
parser.add_argument('--vit_model', type=str, default='vit_base_patch16_224', help='ViT model name from timm.')
parser.add_argument('--checkpoint', type=str, default=None, help='Path to model checkpoint for inference.')
parser.add_argument('--num_samples', type=int, default=8, help='Number of samples to generate in inference mode.')


# --- I-JEPA Specific Args (Used to set up the data loader stubs) ---
parser.add_argument('--crop_size', type=int, default=224, help='Input crop size for ViT and Diffusion.')
parser.add_argument('--patch_size', type=int, default=16, help='Patch size for ViT.')
parser.add_argument('--num_enc_masks', type=int, default=4, help='Number of context blocks (Nenc).')
parser.add_argument('--num_pred_masks', type=int, default=4, help='Number of target blocks (Npred).')
parser.add_argument('--num_workers', type=int, default=4, help='Number of data loading workers.')

args = parser.parse_args()

DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
CHANNELS = 3
VIT_SIZE = args.crop_size
PATCH_SIZE = args.patch_size
logger = DummyLogger()

# --- 1. Architectural Components ---

TIME_EMBED_DIM = 256
BASE_CHANNELS = 32

class TimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )
    def forward(self, t):
        # Sinusoidal Time Encoding
        half_dim = self.dim // 2
        embeddings = torch.log(torch.tensor(10000.0, device=t.device)) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=t.device) * -embeddings)
        embeddings = t[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return self.mlp(embeddings)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, cond_channels):
        super().__init__()
        self.norm1 = nn.GroupNorm(8, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.norm2 = nn.GroupNorm(8, out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.skip_connection = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()
        self.cond_proj = nn.Linear(cond_channels, out_channels) # Projects global condition to feature map size

    def forward(self, x, cond_emb):
        h = x
        h = self.conv1(F.silu(self.norm1(h)))
        # FiLM-style conditioning: modulate feature maps using global latent vector
        cond_scale = self.cond_proj(cond_emb).unsqueeze(-1).unsqueeze(-1)
        h = h + cond_scale # Simple addition for modulation
        h = self.conv2(F.silu(self.norm2(h)))
        return h + self.skip_connection(x)

class ConditionalUNet(nn.Module):
    def __init__(self, image_channels, base_channels, time_dim, vit_latent_dim):
        super().__init__()
        self.time_embed = TimeEmbedding(time_dim)
        
        # Total conditioning dim = Pooled Context Latent + Time Embedding
        cond_dim = vit_latent_dim + time_dim

        self.conv_in = nn.Conv2d(image_channels, base_channels, kernel_size=3, padding=1)
        
        # Downsampling path (224 -> 112 -> 56)
        self.down1 = nn.Sequential(
            ResidualBlock(base_channels, base_channels * 2, cond_dim),
            nn.MaxPool2d(2)
        )
        self.down2 = nn.Sequential(
            ResidualBlock(base_channels * 2, base_channels * 4, cond_dim),
            nn.MaxPool2d(2)
        )

        # Bottleneck (56x56)
        self.bottleneck = ResidualBlock(base_channels * 4, base_channels * 4, cond_dim)

        # Upsampling path (56 -> 112 -> 224)
        self.up1 = nn.Sequential(
            nn.ConvTranspose2d(base_channels * 4, base_channels * 4, kernel_size=2, stride=2), 
            ResidualBlock(base_channels * 8, base_channels * 4, cond_dim) # 4+4=8 for skip connection
        )
        
        self.up2 = nn.Sequential(
            nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2), 
            ResidualBlock(base_channels * 4, base_channels, cond_dim) # 2+2=4 for skip connection
        )
        self.conv_out = nn.Conv2d(base_channels, image_channels, kernel_size=3, padding=1)

    def forward(self, x, t, cond_emb):
        time_emb = self.time_embed(t)
        combined_cond = torch.cat((cond_emb, time_emb), dim=-1) 

        x = self.conv_in(x)
        h1 = self.down1[0](x, combined_cond) 
        x_down1 = self.down1[1](h1) 
        
        h2 = self.down2[0](x_down1, combined_cond) 
        x_down2 = self.down2[1](h2) 

        # Bottleneck
        x = self.bottleneck(x_down2, combined_cond) 

        # Upsampling
        x = self.up1[0](x) 
        x = torch.cat([x, h2], dim=1) # Skip connection
        x = self.up1[1](x, combined_cond) 

        x = self.up2[0](x) 
        x = torch.cat([x, h1], dim=1) # Skip connection
        x = self.up2[1](x, combined_cond) 

        return self.conv_out(x)


class ViTDiffusionModel(nn.Module):
    def __init__(self, image_channels, base_channels, time_dim, vit_model_name, patch_size):
        super().__init__()
        
        # Load ViT and ensure it returns patch features (not just CLS token)
        self.encoder = timm.create_model(
            vit_model_name, 
            pretrained=True, 
            num_classes=0, 
            global_pool='' # Return all patch tokens
        )
        
        # Freeze ViT encoder weights (Acts as the I-JEPA Target/Teacher)
        for param in self.encoder.parameters():
            param.requires_grad = False
            
        vit_latent_dim = self.encoder.embed_dim # D_vit
        
        # Projector to pool the relevant patch tokens into a single global vector
        self.context_pool = nn.Sequential(
            nn.LayerNorm(vit_latent_dim),
            nn.Linear(vit_latent_dim, vit_latent_dim)
        )
        
        # Diffusion Decoder (operates at the full input resolution, e.g., 224x224)
        self.decoder = ConditionalUNet(
            image_channels=image_channels, 
            base_channels=base_channels, 
            time_dim=time_dim, 
            vit_latent_dim=vit_latent_dim
        )

    def get_condition_embedding(self, image, masks_enc):
        """
        Calculates the pooled latent features of the CONTEXT patches.
        
        We simplify by taking the mean of all non-CLS tokens as the global
        conditioning vector, representing the latent information available from
        the full image context.
        """
        # Get all patch tokens (B, N_patches + 1, D_vit)
        tokens = self.encoder.forward_features(image)
        tokens = tokens[:, 1:] # Drop CLS token (B, N_patches, D_vit)
        
        # Global Pooling (Mean across all patches)
        cond_emb = tokens.mean(dim=1) 
        cond_emb = F.gelu(self.context_pool(cond_emb))
        return cond_emb

    def forward(self, x_t, t, full_image, masks_enc, return_cond_emb=False):
        """
        x_t: Noisy image (B, C, H, W)
        t: Timestep (B,)
        full_image: The original image (B, C, H, W) used to generate the context features
        masks_enc: Context mask coordinates (I-JEPA style, used only conceptually here)
        return_cond_emb: If True, returns the conditioning embedding for inference.
        """
        # Get global latent condition from the full image
        cond_emb = self.get_condition_embedding(full_image, masks_enc)
        
        if return_cond_emb:
            return cond_emb
            
        # Predict the noise
        predicted_noise = self.decoder(x_t, t, cond_emb)
        return predicted_noise

# --- Helper function for Target Pixel Masking ---

def create_target_pixel_mask(masks_pred_list, H, W, device):
    """
    Creates a binary pixel mask (B, C, H, W) covering the target blocks.
    Returns: pixel mask where 1.0 is the TARGET (to be predicted) region.
    """
    if not masks_pred_list:
        return torch.ones((1, CHANNELS, H, W), device=device)

    target_blocks = masks_pred_list[0] 
    B = target_blocks.shape[0]
    mask = torch.zeros((B, 1, H, W), device=device)
    
    # Iterate over all target block sets (list of Tensors)
    for target_blocks in masks_pred_list: 
        # target_blocks is (B, N_pred, 4)
        for i in range(B):
            for j in range(target_blocks.shape[1]):
                # Convert normalized coords [0, 1] to pixel coords
                x, y, w, h = target_blocks[i, j].cpu().numpy() 
                x_start = int(x * W)
                y_start = int(y * H)
                x_end = int((x + w) * W)
                y_end = int((y + h) * H)
                
                # Ensure bounds are within image limits
                x_start = max(0, min(x_start, W))
                y_start = max(0, min(y_start, H))
                x_end = max(0, min(x_end, W))
                y_end = max(0, min(y_end, H))
                
                if x_end > x_start and y_end > y_start:
                    mask[i, 0, y_start:y_end, x_start:x_end] = 1.0
    
    return mask.repeat(1, CHANNELS, 1, 1) # Repeat for C channels

def reverse_transform(tensor):
    """Denormalizes and converts a batch tensor to a list of PIL Images."""
    # Denormalization constants for ImageNet
    MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(tensor.device)
    STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(tensor.device)
    
    # Denormalize
    tensor = tensor * STD + MEAN
    
    # Clamp values to [0, 1] and convert to HWC
    tensor = torch.clamp(tensor, 0, 1).cpu()
    
    images = []
    for img_tensor in tensor:
        img = T.ToPILImage()(img_tensor)
        images.append(img)
    return images

def save_images(images, filepath_prefix):
    """Saves a list of images to disk."""
    os.makedirs(os.path.dirname(filepath_prefix), exist_ok=True)
    for i, img in enumerate(images):
        img.save(f"{filepath_prefix}_{i}.png")
        print(f"Saved image to {filepath_prefix}_{i}.png")

# --- 2. Training Function ---

def train_model():
    os.makedirs(args.save_dir, exist_ok=True)
    
    DIFFUSION_IMAGE_SIZE = VIT_SIZE 

    # --- Setup Model and Scheduler ---
    model = ViTDiffusionModel(
        image_channels=CHANNELS,
        base_channels=BASE_CHANNELS,
        time_dim=TIME_EMBED_DIM,
        vit_model_name=args.vit_model,
        patch_size=PATCH_SIZE
    ).to(DEVICE)
    
    scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="linear")
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
    criterion = nn.MSELoss(reduction='none') 

    # --- Setup Data Loader (I-JEPA Style) ---
    transform = make_transforms(
        crop_size=VIT_SIZE,
        crop_scale=[0.08, 1.0], 
        gaussian_blur=False, horizontal_flip=True, color_distortion=True, color_jitter=0.4
    )
    
    mask_collator = MBMaskCollator(
        input_size=VIT_SIZE, patch_size=PATCH_SIZE, 
        pred_mask_scale=[0.15, 0.5], enc_mask_scale=[0.85, 1.0], 
        nenc=args.num_enc_masks, npred=args.num_pred_masks
    )
    
    _, unsupervised_loader, _ = make_imagenet1k(
            transform=transform,
            batch_size=args.batch_size,
            collator=mask_collator,
            pin_mem=True,
            training=True,
            num_workers=args.num_workers,
            world_size=1, rank=0,
            root_path=args.input_dir,
            image_folder='train', 
            copy_data=False,
            drop_last=True)
    
    if not unsupervised_loader:
        print("Error: Data loader failed to initialize or dataset is empty.")
        return

    # --- Training Loop ---
    print(f"Starting training for {args.epochs} epochs. Diffusion Resolution: {DIFFUSION_IMAGE_SIZE}x{DIFFUSION_IMAGE_SIZE}")

    for epoch in range(args.epochs):
        model.train()
        epoch_loss = 0.0
        progress_bar = tqdm(unsupervised_loader, desc=f"Epoch {epoch}/{args.epochs}")

        for step, batch in enumerate(progress_bar):
            try:
                (full_image,), masks_enc_list, masks_pred_list = batch
            except ValueError:
                print("Skipping corrupted batch.")
                continue
                
            full_image = full_image.to(DEVICE, non_blocking=True)
            
            # 1. Sample noise and time steps
            noise = torch.randn_like(full_image)
            B = full_image.shape[0]
            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (B,), device=DEVICE).long()

            # 2. Add noise to the clean image (Forward Diffusion)
            x_t = scheduler.add_noise(full_image, noise, timesteps)

            # 3. Predict noise using the model (conditioned on context latents)
            predicted_noise = model(x_t, timesteps, full_image, masks_enc_list)

            # 4. Create the target pixel mask (B, C, H, W)
            target_pixel_mask = create_target_pixel_mask(
                masks_pred_list, DIFFUSION_IMAGE_SIZE, DIFFUSION_IMAGE_SIZE, DEVICE)

            # 5. Calculate Loss ONLY over the target (masked) regions
            loss = criterion(predicted_noise, noise)
            
            # Apply the mask: only count loss where the mask is 1
            masked_loss = loss * target_pixel_mask
            sum_of_mask_pixels = target_pixel_mask.sum()
            loss = masked_loss.sum() / (sum_of_mask_pixels if sum_of_mask_pixels > 0 else 1.0) 

            # 6. Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            progress_bar.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(unsupervised_loader)
        print(f"\n--- Epoch {epoch} finished. Average Loss: {avg_loss:.4f} ---")

        # --- Save Checkpoint ---
        if (epoch + 1) % 5 == 0 or epoch == args.epochs - 1:
            checkpoint_path = os.path.join(args.save_dir, f"checkpoint_epoch_{epoch+1}.pth")
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)
            print(f"Saved checkpoint to {checkpoint_path}")

    print("\nTraining complete.")

# --- 3. Inference Function ---

@torch.no_grad()
def inference_model():
    if not args.checkpoint:
        print("Error: Must provide a --checkpoint path for inference mode.")
        return

    DIFFUSION_IMAGE_SIZE = VIT_SIZE 
    
    # --- Setup Model and Scheduler ---
    model = ViTDiffusionModel(
        image_channels=CHANNELS,
        base_channels=BASE_CHANNELS,
        time_dim=TIME_EMBED_DIM,
        vit_model_name=args.vit_model,
        patch_size=PATCH_SIZE
    ).to(DEVICE)
    
    # Load checkpoint
    print(f"Loading checkpoint from {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule="linear")

    # --- Setup Data Loader for one batch of samples ---
    transform = make_transforms(
        crop_size=VIT_SIZE,
        crop_scale=[0.08, 1.0], 
        gaussian_blur=False, horizontal_flip=True, color_distortion=True, color_jitter=0.4
    )
    
    mask_collator = MBMaskCollator(
        input_size=VIT_SIZE, patch_size=PATCH_SIZE, 
        pred_mask_scale=[0.15, 0.5], enc_mask_scale=[0.85, 1.0], 
        nenc=args.num_enc_masks, npred=args.num_pred_masks
    )
    
    # Use batch_size = num_samples for inference run
    _, inference_loader, _ = make_imagenet1k(
            transform=transform,
            batch_size=args.num_samples, # Load the desired number of samples
            collator=mask_collator,
            pin_mem=True,
            training=False,
            num_workers=args.num_workers,
            world_size=1, rank=0,
            root_path=args.input_dir,
            image_folder='train', 
            copy_data=False,
            drop_last=True)
            
    if not inference_loader:
        print("Error: Data loader failed to initialize for inference. Check if --input_dir contains images.")
        return

    # Get one batch for inference
    try:
        (full_image,), masks_enc_list, masks_pred_list = next(iter(inference_loader))
    except StopIteration:
        print("Error: Data loader yielded no samples.")
        return
        
    full_image = full_image.to(DEVICE, non_blocking=True)
    B = full_image.shape[0]

    # 1. Get the conditioning latent vector
    cond_emb = model(None, None, full_image, masks_enc_list, return_cond_emb=True)
    
    # 2. Create the target and context pixel masks
    target_mask = create_target_pixel_mask(
        masks_pred_list, DIFFUSION_IMAGE_SIZE, DIFFUSION_IMAGE_SIZE, DEVICE)
    context_mask = 1.0 - target_mask
    
    # 3. Create the initial noisy input image (x_T)
    # Start with full noise
    x = torch.randn_like(full_image) 
    
    # Noise the clean image to a high timestep (t=999) for the context region reference
    t_start = torch.full((B,), scheduler.config.num_train_timesteps - 1, device=DEVICE).long()
    context_reference = scheduler.add_noise(full_image, torch.randn_like(full_image), t_start)
    
    # Combine: Noisy target region, Noisy context region (high t)
    # The initial noise for context is taken from the noised original image
    x = x * target_mask + context_reference * context_mask
    
    print(f"Starting {scheduler.config.num_train_timesteps} step inpainting...")
    
    # 4. Reverse Diffusion Loop (Inpainting)
    for t in tqdm(reversed(range(scheduler.config.num_train_timesteps)), desc="Inpainting"):
        t_tensor = torch.full((B,), t, device=DEVICE).long()
        
        # Predict the noise for the current step t
        predicted_noise = model(x, t_tensor, full_image, masks_enc_list)

        # Calculate the predicted x_{t-1} (one step less noisy)
        model_output = scheduler.step(predicted_noise, t, x, generator=None)
        x_pred = model_output.prev_sample
        
        # 5. Inpainting Step: Enforce the context region (known pixels)
        if t > 0:
            # We enforce the context to be the original image's context region 
            # (which is also appropriately noised for step t-1).
            # For simplicity in this implementation, we take the clean original image
            # for the context mask, which makes it a strict inpainting setup.
            x = x_pred * target_mask + full_image * context_mask
        else:
            # At t=0, x_pred is the final, clean prediction
            x = x_pred
            
    # 6. Visualization and Saving
    
    # Denormalize all images
    original_images = reverse_transform(full_image)
    predicted_images = reverse_transform(x)
    
    # Create masked images for visualization (context only)
    # Set masked areas (target region) to a dark color for visibility
    # We use a normalized value of 0.0 (grey) for the unmasked region
    
    # Denormalize the original image for the context region
    denorm_full_image = full_image * torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1) + torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)
    
    # Create the masked input by taking the context from the denormalized image, 
    # and setting the target region to a placeholder color (e.g., black or gray, represented by 0.0)
    placeholder_color = 0.1 # Dark gray
    masked_input_tensor = denorm_full_image * context_mask + placeholder_color * target_mask
    
    masked_input_images = reverse_transform(masked_input_tensor)

    # Save results
    save_dir_inference = os.path.join(args.save_dir, "inference_results")
    os.makedirs(save_dir_inference, exist_ok=True)
    
    save_images(original_images, os.path.join(save_dir_inference, "original"))
    save_images(masked_input_images, os.path.join(save_dir_inference, "masked_input"))
    save_images(predicted_images, os.path.join(save_dir_inference, "prediction"))
    
    print("\nInference complete. Check the 'inference_results' directory.")


# --- 4. Main Execution ---

if __name__ == '__main__':
    if args.mode == 'train':
        train_model()
    elif args.mode == 'inference':
        inference_model()
